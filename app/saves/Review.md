# Review

## Discrete Random Variabe

1. Entropy

2. Divergence
   $$
   \pi(x|x^n) = q(x)\\
   \begin{array}{lll}
   log(\prod p(x)^{nq(x)}) &amp;amp;amp; = &amp;amp;amp; n\sum q(x)log \space p(x) \\
    &amp;amp;amp; = &amp;amp;amp; n(-D(q||p) + \sum q(x)log \space q(x))
   \end{array}&nbsp;
   $$

3. Mutual Information

   included by Entropy and Divergence, some properties

## Source Coding

1. Zero error compression

   One-on-one projection $f:x\rightarrow M$

   * Huffman Code
   * Shannon Code ($\bar{L} \leq H(x) + 1$)

2. Lossless Compression $P(error) \rightarrow 0$

   Typical Set$T^{(n)}_{\epsilon}$

   * Weak typicality: depend on $p(x^n)$

   * Strong typicality: depend on $\pi(x|x^n)$

     $Pr(TypicalSet) \rightarrow 1$ according to **L.L.N**

   1. $T^{(n)}_{\epsilon} \approx 2^{nH(X)}$
   2. $x^n \in T^{(n)}_{\epsilon}, p(x^n) \approx 2^{-nH(X)}$ bounded on empirial distribution.

3. Rate distortion

   $E(d(x,\hat{x})) \leq D, R \geq I(X;\hat{X})$

   $\rightarrow​$ Joint Typical Set $T^{(n)}_{\epsilon}​$

   1. $|T^{(n)}_{\epsilon}(X,Y)| \approx 2^{nH(X,Y)}​$

   2. $Pr(T^{(n)}_{\epsilon}(X,Y)) \approx 1​$

   3. $(x^n,y^n) \in T^{(n)}_{\epsilon}(X,Y), p(x^n,y^n) \approx 2^{-nH(X,Y)}$

   4. $(x^n,y^n)$ generated by $p(x),p(y)$, $Pr\{(x^n,y^n) \in T^{(n)}_{\epsilon}(X,Y) \} \approx 2^{-nI(X,Y)}$
      $$
      \begin{array}{lll}
      Pr\{(x^n,y^n) \in T^{(n)}_{\epsilon}(X,Y) \} &amp;amp;amp; = &amp;amp;amp; \sum\limits_{(x^n,y^n) \in T^{(n)}_{\epsilon}(X,Y)} p(x^n,y^n) \\
       &amp;amp;amp; = &amp;amp;amp; \sum\limits_{(x^n,y^n) \in T^{(n)}_{\epsilon}(X,Y)} p(x^n)p(y^n) \\
      
      \end{array}
      $$
      

   5. Given $x^n$, generate $y^n$ according to $p(y)$
      $$
      Pr\{(x^n,y^n) \in T^{(n)}_{\epsilon}(X,Y)\}
      $$

   6. Joint typical Lemma
      * Packing Lemma
      * Covering Lemma




$$
\begin{array}{lll}
I(X;Y|Z) &amp;amp;amp; = &amp;amp;amp; \sum\limits_zp(z) I(X;Y|Z=z)\\
 &amp;amp;amp; = &amp;amp;amp; \sum\limits_zp(z)\sum\limits_{x,y}p(x,y|z)log\frac{p(x,y|z)}{p(x|z)p(y|z)}\\
 &amp;amp;amp; = &amp;amp;amp; I(X;Y,Z) - I(X;Z) \\
 &amp;amp;amp; = &amp;amp;amp; I(X,Z;Y) - I(Y;Z)\\
 &amp;amp;amp; = &amp;amp;amp; H(X|Z) - H(X|Y,Z)\\
 I(X;Y|W,Z) &amp;amp;amp; = &amp;amp;amp; I(X;Y,W|Z) - I(X;W|Z)\\
 
\end{array}
$$


## 信道编码

### 信道容量

在n次使用信道的条件下，将计算出可区分的信号的最大数目。该数与n成指数增长的关系，这个指数就是所说的**信道容量**。而信道容量又被被特征化成为了**最大互信息**。

 ### 离散信道（discret channel）

由输入字母表$\mathcal{X}$, 输出字母表$\mathcal{Y}$ 和转移矩阵$p(y|x)$构成的系统。如果输出的概率分布仅仅依赖于它所对应的输入，而与先前信道的输入或者输出条件独立，就称这个信道是**无记忆的（memoryless）**

### 离散无记忆信道

离散无记忆信道的“信息”信道容量定义为
$$
C = \mathop{max}\limits_{p(x)}I(X;Y)
$$
