{"notes":[{"title":"Review","lastOpen":1559798623825,"TotalTime":3964903726,"words":2284,"path":"D:\\Projects\\XNote\\app\\saves\\Review.md","content":"# Review\n\n## Discrete Random Variabe\n\n1. Entropy\n\n2. Divergence\n   $$\n   \\pi(x|x^n) = q(x)\\\\\n   \\begin{array}{lll}\n   log(\\prod p(x)^{nq(x)}) &amp;amp;amp; = &amp;amp;amp; n\\sum q(x)log \\space p(x) \\\\\n    &amp;amp;amp; = &amp;amp;amp; n(-D(q||p) + \\sum q(x)log \\space q(x))\n   \\end{array}&nbsp;\n   $$\n\n3. Mutual Information\n\n   included by Entropy and Divergence, some properties\n\n## Source Coding\n\n1. Zero error compression\n\n   One-on-one projection $f:x\\rightarrow M$\n\n   * Huffman Code\n   * Shannon Code ($\\bar{L} \\leq H(x) + 1$)\n\n2. Lossless Compression $P(error) \\rightarrow 0$\n\n   Typical Set$T^{(n)}_{\\epsilon}$\n\n   * Weak typicality: depend on $p(x^n)$\n\n   * Strong typicality: depend on $\\pi(x|x^n)$\n\n     $Pr(TypicalSet) \\rightarrow 1$ according to **L.L.N**\n\n   1. $T^{(n)}_{\\epsilon} \\approx 2^{nH(X)}$\n   2. $x^n \\in T^{(n)}_{\\epsilon}, p(x^n) \\approx 2^{-nH(X)}$ bounded on empirial distribution.\n\n3. Rate distortion\n\n   $E(d(x,\\hat{x})) \\leq D, R \\geq I(X;\\hat{X})$\n\n   $\\rightarrow​$ Joint Typical Set $T^{(n)}_{\\epsilon}​$\n\n   1. $|T^{(n)}_{\\epsilon}(X,Y)| \\approx 2^{nH(X,Y)}​$\n\n   2. $Pr(T^{(n)}_{\\epsilon}(X,Y)) \\approx 1​$\n\n   3. $(x^n,y^n) \\in T^{(n)}_{\\epsilon}(X,Y), p(x^n,y^n) \\approx 2^{-nH(X,Y)}$\n\n   4. $(x^n,y^n)$ generated by $p(x),p(y)$, $Pr\\{(x^n,y^n) \\in T^{(n)}_{\\epsilon}(X,Y) \\} \\approx 2^{-nI(X,Y)}$\n      $$\n      \\begin{array}{lll}\n      Pr\\{(x^n,y^n) \\in T^{(n)}_{\\epsilon}(X,Y) \\} &amp;amp;amp; = &amp;amp;amp; \\sum\\limits_{(x^n,y^n) \\in T^{(n)}_{\\epsilon}(X,Y)} p(x^n,y^n) \\\\\n       &amp;amp;amp; = &amp;amp;amp; \\sum\\limits_{(x^n,y^n) \\in T^{(n)}_{\\epsilon}(X,Y)} p(x^n)p(y^n) \\\\\n      \n      \\end{array}\n      $$\n      \n\n   5. Given $x^n$, generate $y^n$ according to $p(y)$\n      $$\n      Pr\\{(x^n,y^n) \\in T^{(n)}_{\\epsilon}(X,Y)\\}\n      $$\n\n   6. Joint typical Lemma\n      * Packing Lemma\n      * Covering Lemma\n\n\n\n\n$$\n\\begin{array}{lll}\nI(X;Y|Z) &amp;amp;amp; = &amp;amp;amp; \\sum\\limits_zp(z) I(X;Y|Z=z)\\\\\n &amp;amp;amp; = &amp;amp;amp; \\sum\\limits_zp(z)\\sum\\limits_{x,y}p(x,y|z)log\\frac{p(x,y|z)}{p(x|z)p(y|z)}\\\\\n &amp;amp;amp; = &amp;amp;amp; I(X;Y,Z) - I(X;Z) \\\\\n &amp;amp;amp; = &amp;amp;amp; I(X,Z;Y) - I(Y;Z)\\\\\n &amp;amp;amp; = &amp;amp;amp; H(X|Z) - H(X|Y,Z)\\\\\n I(X;Y|W,Z) &amp;amp;amp; = &amp;amp;amp; I(X;Y,W|Z) - I(X;W|Z)\\\\\n \n\\end{array}\n$$\n\n\n## 信道编码\n\n### 信道容量\n\n在n次使用信道的条件下，将计算出可区分的信号的最大数目。该数与n成指数增长的关系，这个指数就是所说的**信道容量**。而信道容量又被被特征化成为了**最大互信息**。\n\n ### 离散信道（discret channel）\n\n由输入字母表$\\mathcal{X}$, 输出字母表$\\mathcal{Y}$ 和转移矩阵$p(y|x)$构成的系统。如果输出的概率分布仅仅依赖于它所对应的输入，而与先前信道的输入或者输出条件独立，就称这个信道是**无记忆的（memoryless）**\n\n### 离散无记忆信道\n\n离散无记忆信道的“信息”信道容量定义为\n$$\nC = \\mathop{max}\\limits_{p(x)}I(X;Y)\n$$\n","desc":"No description."},{"TotalTime":1979199,"content":"","desc":"No description.","lastOpen":1559797916078,"path":"D:\\Projects\\XNote\\app\\saves\\newtest.md","title":"# # new test","words":9},{"title":"hw4","lastOpen":1559446553084,"TotalTime":593554433,"words":74,"path":"D:\\Projects\\XNote\\app\\saves\\hw4.md","content":"","desc":"No description."},{"title":"gtor","lastOpen":1558424220157,"TotalTime":11385,"words":0,"path":"D:\\Projects\\XNote\\app\\saves\\gtor.md","content":"","desc":"No description."},{"title":"test","lastOpen":1558422742733,"TotalTime":42166,"words":56,"path":"D:\\Projects\\XNote\\app\\saves\\test.txt","content":"","desc":"No description."},{"title":"buffer","lastOpen":1558422635337,"TotalTime":72818,"words":2609,"path":"D:\\Projects\\XNote\\app\\saves\\buffer.md","content":"","desc":"No description."}]}